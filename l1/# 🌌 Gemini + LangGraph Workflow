## ðŸŽ¯ Objective

Experimenting with **LangGraph** workflow engine and integrating it with **Google Gemini API** for simple LLM Q\&A.

## ðŸ› ï¸ Setup & Tools

* **LangGraph** â†’ for building stateful workflows
* **Gemini API (google-generativeai)** â†’ as the LLM backend
* **Python + dotenv** â†’ to manage environment variables (`.env` file with `GEMINI_API_KEY`)

## âš¡ Workflow Explained

1. **Define State** â€“ We use a `TypedDict` to store `question` and `answer`.
2. **LLM Node** â€“ A function `llm_qa` that queries Gemini with the question.
3. **Graph Construction** â€“ Using `StateGraph`, we connect:

   ```
   START â†’ llm_qa â†’ END
   ```
4. **Execution** â€“ Pass a question like *â€œHow far is the moon from Earth?â€* and get the Gemini-generated answer.

## ðŸž Key Learnings

* **Jupyter JSON cells â‰  Python script** â†’ accidentally copied `null` values which broke the script.
* `google.generativeai` has no `Client()` â†’ instead, we use:

  ```python
  model = genai.GenerativeModel("gemini-1.5-flash")
  response = model.generate_content("your prompt here")
  ```
* Debugging environment variables with `print(os.environ.get("GEMINI_API_KEY"))` is super helpful.

## âœ… Final Output

Example run:

```bash
> python gemini_workflow.py
The moon is about 384,400 km away from Earth.
```

## ðŸš€ Next Steps

* Add multiple nodes (e.g., summarization â†’ translation â†’ answer).
* Build a chatbot loop with memory.
* Try LangChain + LangGraph hybrid workflows.

